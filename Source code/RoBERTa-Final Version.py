# -*- coding: utf-8 -*-
"""FinalVersionRoberta

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E5xpRGg4An2ABecH-nKh6I_PybQbdDiY
"""

!pip install -q transformers opendatasets emoji scikit-learn optuna

import os, re, time, random, json
import emoji
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from torch.optim import AdamW

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    classification_report, confusion_matrix, ConfusionMatrixDisplay
)

import opendatasets as od
od.download(
    "https://www.kaggle.com/datasets/sandhyapeesara/cyberbullying-detection-dataset",
    data_dir=".",
    force=True
)

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Available Device:", device)

def textFixNormlize(text):
    text = str(text).lower()
    text = re.sub(r"\S+@\S+\.\S+", "", text)     # remove emails
    text = re.sub(r'@\w+', '', text)            # remove mentions
    text = emoji.demojize(text, delimiters=(" ", " "))
    text = re.sub(r'#(\w+)', r'\1', text)       # remove hashtag symbol only
    text = re.sub(r'\s+', ' ', text).strip()
    return text

data_df = pd.read_json(
    "/content/cyberbullying-detection-dataset/Cyber_Bully_Data.json",
    lines=True
)

data_df["clean_text"] = data_df["text"].astype(str).apply(textFixNormlize)

le = LabelEncoder()
data_df["label_id"] = le.fit_transform(data_df["label"])

data_df.dropna(subset=["clean_text", "label_id"], inplace=True)
data_df.drop_duplicates(subset=["clean_text", "label_id"], inplace=True)

print("Shape after cleaning:", data_df.shape)
print("Labels:", list(le.classes_))
print("\nClass distribution:")
print(data_df["label"].value_counts())

MODEL_NAME = "roberta-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

data_df["char_len"] = data_df["clean_text"].str.len()
print("\nCharacter length stats:")
print(data_df["char_len"].describe())

data_df["tok_len"] = data_df["clean_text"].apply(lambda x: len(tokenizer.tokenize(str(x))))
print("\nToken length stats:")
print(data_df["tok_len"].describe())

plt.figure(figsize=(7,4))
plt.hist(data_df["tok_len"], bins=50)
plt.title("Token length distribution (RoBERTa tokenizer)")
plt.xlabel("Tokens")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

X = np.array(data_df["clean_text"])
y = np.array(data_df["label_id"])

X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.50, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp
)

print(f"Train: {len(X_train)/len(X)*100:.2f}%")
print(f"Val:   {len(X_val)/len(X)*100:.2f}%")
print(f"Test:  {len(X_test)/len(X)*100:.2f}%")

class MyDataset(Dataset):
    def __init__(self, X, Y, tokenizer, max_len):
        self.X = X
        self.Y = Y
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        text = str(self.X[idx])
        label = int(self.Y[idx])

        enc = self.tokenizer(
            text,
            max_length=self.max_len,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        return {
            "input_ids": enc["input_ids"].squeeze(0),
            "attention_mask": enc["attention_mask"].squeeze(0),
            "labels": torch.tensor(label, dtype=torch.long)
        }

class MyModel(nn.Module):
    def __init__(self, base_model, num_classes, dropout=0.30):
        super().__init__()
        self.base = base_model
        self.dropout1 = nn.Dropout(dropout)
        self.fc1 = nn.Linear(768, 256)   # roberta-base hidden size = 768
        self.act = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)
        self.fc2 = nn.Linear(256, num_classes)

    def forward(self, input_ids, attention_mask):
        out = self.base(input_ids=input_ids, attention_mask=attention_mask)
        pooled = out.last_hidden_state[:, 0]  # <s> token embedding (RoBERTa)
        x = self.dropout1(pooled)
        x = self.fc1(x)
        x = self.act(x)
        x = self.dropout2(x)
        return self.fc2(x)

def run_roberta_experiment(
    run_name="roberta_run",
    max_len=100,
    batch_size=32,
    epochs=3,
    lr=2e-5,
    dropout=0.30,
    unfreeze_last_n=2
):
    # Fresh tokenizer/model for fair runs
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    base_model = AutoModel.from_pretrained(MODEL_NAME)

    num_classes = len(le.classes_)

    # DataLoaders
    train_loader = DataLoader(
        MyDataset(X_train, y_train, tokenizer, max_len),
        batch_size=batch_size,
        shuffle=True
    )
    val_loader = DataLoader(
        MyDataset(X_val, y_val, tokenizer, max_len),
        batch_size=batch_size,
        shuffle=False
    )
    test_loader = DataLoader(
        MyDataset(X_test, y_test, tokenizer, max_len),
        batch_size=batch_size,
        shuffle=False
    )

    # Class weights (helps imbalance)
    classes = np.unique(y_train)
    cw = compute_class_weight(class_weight="balanced", classes=classes, y=y_train)
    cw = torch.tensor(cw, dtype=torch.float).to(device)
    criterion = nn.CrossEntropyLoss(weight=cw)

    # Build model
    model = MyModel(base_model, num_classes, dropout=dropout).to(device)

    # Freeze all then unfreeze last N transformer layers
    for p in model.base.parameters():
        p.requires_grad = False
    for p in model.base.encoder.layer[-unfreeze_last_n:].parameters():
        p.requires_grad = True

    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)

    history = {"train_loss": [], "val_loss": [], "train_acc": [], "val_acc": []}
    best_val_acc = -1.0
    best_state_dict = None

    start_time = time.time()

    for epoch in range(epochs):
        # ---- TRAIN
        model.train()
        train_loss_sum, train_correct, train_total = 0.0, 0, 0

        for batch in train_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            optimizer.zero_grad()
            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()

            train_loss_sum += loss.item() * labels.size(0)
            preds = torch.argmax(logits, dim=1)
            train_correct += (preds == labels).sum().item()
            train_total += labels.size(0)

        avg_train_loss = train_loss_sum / train_total
        avg_train_acc = train_correct / train_total

        # ---- VAL
        model.eval()
        val_loss_sum, val_correct, val_total = 0.0, 0, 0

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)

                logits = model(input_ids, attention_mask)
                loss = criterion(logits, labels)

                val_loss_sum += loss.item() * labels.size(0)
                preds = torch.argmax(logits, dim=1)
                val_correct += (preds == labels).sum().item()
                val_total += labels.size(0)

        avg_val_loss = val_loss_sum / val_total
        avg_val_acc = val_correct / val_total

        history["train_loss"].append(avg_train_loss)
        history["val_loss"].append(avg_val_loss)
        history["train_acc"].append(avg_train_acc)
        history["val_acc"].append(avg_val_acc)

        print(
            f"[{run_name}] Epoch {epoch+1}/{epochs} | "
            f"Train Loss {avg_train_loss:.4f} Acc {avg_train_acc*100:.2f}% | "
            f"Val Loss {avg_val_loss:.4f} Acc {avg_val_acc*100:.2f}%"
        )

        # Keep best model in memory (no saving)
        if avg_val_acc > best_val_acc:
            best_val_acc = avg_val_acc
            best_state_dict = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
            print(f"✅ Best model updated in memory (Val Acc {best_val_acc*100:.2f}%)")

    train_time_sec = time.time() - start_time
    print(f"[{run_name}] Training time: {train_time_sec:.2f} seconds")

    # ---- Load best weights from memory
    if best_state_dict is not None:
        model.load_state_dict(best_state_dict)
    model = model.to(device)
    model.eval()

    # ---- TEST
    all_preds, all_labels = [], []
    test_loss_sum, test_correct, test_total = 0.0, 0, 0

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            logits = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = criterion(logits, labels)

            test_loss_sum += loss.item() * labels.size(0)

            preds = torch.argmax(logits, dim=1)
            test_correct += (preds == labels).sum().item()
            test_total += labels.size(0)

            all_preds.extend(preds.cpu().numpy().tolist())
            all_labels.extend(labels.cpu().numpy().tolist())

    avg_test_loss = test_loss_sum / test_total
    avg_test_acc = test_correct / test_total

    macro_f1 = f1_score(all_labels, all_preds, average="macro")
    weighted_f1 = f1_score(all_labels, all_preds, average="weighted")
    macro_prec = precision_score(all_labels, all_preds, average="macro", zero_division=0)
    macro_rec = recall_score(all_labels, all_preds, average="macro", zero_division=0)

    metrics = {
        "run_name": run_name,
        "best_val_acc": float(best_val_acc),
        "test_loss": float(avg_test_loss),
        "test_acc": float(avg_test_acc),
        "macro_f1": float(macro_f1),
        "weighted_f1": float(weighted_f1),
        "macro_precision": float(macro_prec),
        "macro_recall": float(macro_rec),
        "training_time_sec": float(train_time_sec),
        "config": {
            "model_name": MODEL_NAME,
            "max_len": max_len,
            "batch_size": batch_size,
            "epochs": epochs,
            "lr": lr,
            "dropout": dropout,
            "unfreeze_last_n": unfreeze_last_n
        }
    }

    print(f"\n✅ TEST RESULTS | Loss: {avg_test_loss:.4f} | Accuracy: {avg_test_acc*100:.2f}%\n")
    print("Classification Report:")
    print(classification_report(all_labels, all_preds, target_names=le.classes_))

    cm = confusion_matrix(all_labels, all_preds)

    plt.figure(figsize=(7, 6))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
    disp.plot(values_format="d", cmap="Blues")
    plt.title(f"Confusion Matrix (Counts) — {run_name}")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    return metrics, history

baseline_metrics, baseline_history = run_roberta_experiment(
    run_name="roberta_baseline",
    max_len=100,
    batch_size=32,
    epochs=3,
    lr=2e-5,
    dropout=0.30,
    unfreeze_last_n=2,

)

manual_space = [
    {"lr": 2e-5, "max_len": 100, "dropout": 0.30},
    {"lr": 3e-5, "max_len": 100, "dropout": 0.30},
    {"lr": 2e-5, "max_len": 128, "dropout": 0.30},
    {"lr": 2e-5, "max_len": 100, "dropout": 0.40},
]

manual_results = []
for i, cfg in enumerate(manual_space, start=1):
    set_seed(42)
    run_name = f"roberta_manual_{i}_lr{cfg['lr']}_len{cfg['max_len']}_do{cfg['dropout']}"
    metrics, _ = run_roberta_experiment(
        run_name=run_name,
        max_len=cfg["max_len"],
        batch_size=32,
        epochs=2,
        lr=cfg["lr"],
        dropout=cfg["dropout"],
        unfreeze_last_n=2,

    )
    manual_results.append(metrics)

manual_df = pd.DataFrame(manual_results).sort_values("best_val_acc", ascending=False)
manual_df[["run_name","best_val_acc","test_acc","macro_f1","training_time_sec"]]

import optuna

def objective(trial):
    set_seed(42)

    lr = trial.suggest_float("lr", 1e-5, 5e-5, log=True)
    max_len = trial.suggest_categorical("max_len", [100, 128])
    dropout = trial.suggest_float("dropout", 0.20, 0.50)

    run_name = f"roberta_optuna_lr{lr:.2e}_len{max_len}_do{dropout:.2f}"
    metrics, _ = run_roberta_experiment(
        run_name=run_name,
        max_len=max_len,
        batch_size=32,
        epochs=2,
        lr=lr,
        dropout=dropout,
        unfreeze_last_n=2,

    )
    return metrics["best_val_acc"]

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=3)  # increase to 8–12 if you want

print("Best params:", study.best_params)
print("Best best_val_acc:", study.best_value)

best_params = {
    "lr": 2.342457631106799e-05,
    "max_len": 128,
    "dropout": 0.23060674293378702
}

set_seed(42)

final_metrics, final_history = run_roberta_experiment(
    run_name="roberta_final_best",
    max_len=best_params["max_len"],
    batch_size=32,
    epochs=5,
    lr=best_params["lr"],
    dropout=best_params["dropout"],
    unfreeze_last_n=2
)