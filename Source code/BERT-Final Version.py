# -*- coding: utf-8 -*-
"""FinalVersionBert

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jj08pDIreUIHVgRp-PpszAgIGxwFAeJb
"""

!pip install -q transformers opendatasets emoji scikit-learn optuna

import os, re, time, random, json
import emoji
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from torch.optim import AdamW

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    classification_report, confusion_matrix, ConfusionMatrixDisplay
)

import opendatasets as od
od.download(
    "https://www.kaggle.com/datasets/sandhyapeesara/cyberbullying-detection-dataset",
    data_dir=".",
    force=True
)

import os
import random
import numpy as np
import torch

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Available Device:", device)

def textFixNormlize(text):
    text = str(text).lower()
    text = re.sub(r"\S+@\S+\.\S+", "", text)     # remove emails
    text = re.sub(r'@\w+', '', text)            # remove mentions
    text = emoji.demojize(text, delimiters=(" ", " "))
    text = re.sub(r'#(\w+)', r'\1', text)       # remove hashtag symbol only
    text = re.sub(r'\s+', ' ', text).strip()
    return text

data_df = pd.read_json(
    "/content/cyberbullying-detection-dataset/Cyber_Bully_Data.json",
    lines=True
)

data_df["clean_text"] = data_df["text"].astype(str).apply(textFixNormlize)

le = LabelEncoder()
data_df["label_id"] = le.fit_transform(data_df["label"])

data_df.dropna(subset=["clean_text", "label_id"], inplace=True)
data_df.drop_duplicates(subset=["clean_text", "label_id"], inplace=True)

print("Shape after cleaning:", data_df.shape)
print("Labels:", list(le.classes_))
data_df.head()

MODEL_NAME = "google-bert/bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

print("Class distribution:")
print(data_df["label"].value_counts())

data_df["char_len"] = data_df["clean_text"].str.len()
print("\nCharacter length stats:")
print(data_df["char_len"].describe())

data_df["tok_len"] = data_df["clean_text"].apply(lambda x: len(tokenizer.tokenize(str(x))))
print("\nToken length stats:")
print(data_df["tok_len"].describe())

plt.figure(figsize=(7,4))
plt.hist(data_df["tok_len"], bins=50)
plt.title("Token length distribution")
plt.xlabel("Tokens")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

X = np.array(data_df["clean_text"])
y = np.array(data_df["label_id"])

X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.50, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp
)

print(f"Train: {len(X_train)/len(X)*100:.2f}%")
print(f"Val:   {len(X_val)/len(X)*100:.2f}%")
print(f"Test:  {len(X_test)/len(X)*100:.2f}%")

class MyDataset(Dataset):
    def __init__(self, X, Y, tokenizer, max_len):
        self.X = X
        self.Y = Y
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        text = str(self.X[idx])
        label = int(self.Y[idx])

        enc = self.tokenizer(
            text,
            max_length=self.max_len,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        return {
            "input_ids": enc["input_ids"].squeeze(0),
            "attention_mask": enc["attention_mask"].squeeze(0),
            "labels": torch.tensor(label, dtype=torch.long)
        }

class MyModel(nn.Module):
    def __init__(self, base_model, num_classes, dropout=0.30):
        super().__init__()
        self.base = base_model
        self.dropout1 = nn.Dropout(dropout)
        self.fc1 = nn.Linear(768, 256)
        self.act = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)
        self.fc2 = nn.Linear(256, num_classes)

    def forward(self, input_ids, attention_mask):
        out = self.base(input_ids=input_ids, attention_mask=attention_mask)
        cls = out.last_hidden_state[:, 0]
        x = self.dropout1(cls)
        x = self.fc1(x)
        x = self.act(x)
        x = self.dropout2(x)
        return self.fc2(x)

def run_bert_experiment(
    run_name="bert_run",
    max_len=100,
    batch_size=32,
    epochs=3,
    lr=2e-5,
    dropout=0.30,
    unfreeze_last_n=2,
    save_dir="results_bert"
):
    os.makedirs(save_dir, exist_ok=True)

    # Fresh tokenizer/model for fair runs
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    base_model = AutoModel.from_pretrained(MODEL_NAME)

    num_classes = len(le.classes_)

    # DataLoaders
    train_loader = DataLoader(MyDataset(X_train, y_train, tokenizer, max_len), batch_size=batch_size, shuffle=True)
    val_loader   = DataLoader(MyDataset(X_val, y_val, tokenizer, max_len), batch_size=batch_size, shuffle=False)
    test_loader  = DataLoader(MyDataset(X_test, y_test, tokenizer, max_len), batch_size=batch_size, shuffle=False)

    # Class weights (helps imbalance)
    classes = np.unique(y_train)
    cw = compute_class_weight(class_weight="balanced", classes=classes, y=y_train)
    cw = torch.tensor(cw, dtype=torch.float).to(device)
    criterion = nn.CrossEntropyLoss(weight=cw)

    model = MyModel(base_model, num_classes, dropout=dropout).to(device)

    # Freeze all then unfreeze last N transformer layers
    for p in model.base.parameters():
        p.requires_grad = False
    for p in model.base.encoder.layer[-unfreeze_last_n:].parameters():
        p.requires_grad = True

    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)

    history = {"train_loss": [], "val_loss": [], "train_acc": [], "val_acc": []}
    best_val_acc = -1.0
    best_ckpt_path = os.path.join(save_dir, f"{run_name}_best.pt")

    # Timing (7)
    start_time = time.time()

    for epoch in range(epochs):
        # ---- TRAIN
        model.train()
        train_loss_sum, train_correct, train_total = 0.0, 0, 0

        for batch in train_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            optimizer.zero_grad()
            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()

            train_loss_sum += loss.item() * labels.size(0)
            preds = torch.argmax(logits, dim=1)
            train_correct += (preds == labels).sum().item()
            train_total += labels.size(0)

        avg_train_loss = train_loss_sum / train_total
        avg_train_acc = train_correct / train_total

        # ---- VAL
        model.eval()
        val_loss_sum, val_correct, val_total = 0.0, 0, 0

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)

                logits = model(input_ids, attention_mask)
                loss = criterion(logits, labels)

                val_loss_sum += loss.item() * labels.size(0)
                preds = torch.argmax(logits, dim=1)
                val_correct += (preds == labels).sum().item()
                val_total += labels.size(0)

        avg_val_loss = val_loss_sum / val_total
        avg_val_acc = val_correct / val_total

        history["train_loss"].append(avg_train_loss)
        history["val_loss"].append(avg_val_loss)
        history["train_acc"].append(avg_train_acc)
        history["val_acc"].append(avg_val_acc)

        print(f"[{run_name}] Epoch {epoch+1}/{epochs} | "
              f"Train Loss {avg_train_loss:.4f} Acc {avg_train_acc*100:.2f}% | "
              f"Val Loss {avg_val_loss:.4f} Acc {avg_val_acc*100:.2f}%")

        # Best checkpoint (4)
        if avg_val_acc > best_val_acc:
            best_val_acc = avg_val_acc
            torch.save({
                "model_state": model.state_dict(),
                "label_classes": le.classes_,
                "config": {
                    "model_name": MODEL_NAME,
                    "max_len": max_len,
                    "batch_size": batch_size,
                    "epochs": epochs,
                    "lr": lr,
                    "dropout": dropout,
                    "unfreeze_last_n": unfreeze_last_n
                }
            }, best_ckpt_path)
            print(f"✅ Saved BEST checkpoint: {best_ckpt_path} (Val Acc {best_val_acc*100:.2f}%)")

    train_time_sec = time.time() - start_time
    print(f"[{run_name}] Training time: {train_time_sec:.2f} seconds")

    # ---- Load best checkpoint
    ckpt = torch.load(best_ckpt_path, map_location=device, weights_only=False)
    model.load_state_dict(ckpt["model_state"])
    model.eval()

    # ---- TEST (3)
    all_preds, all_labels = [], []
    test_loss_sum, test_correct, test_total = 0.0, 0, 0

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            logits = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = criterion(logits, labels)

            test_loss_sum += loss.item() * labels.size(0)

            preds = torch.argmax(logits, dim=1)
            test_correct += (preds == labels).sum().item()
            test_total += labels.size(0)

            all_preds.extend(preds.cpu().numpy().tolist())
            all_labels.extend(labels.cpu().numpy().tolist())

    avg_test_loss = test_loss_sum / test_total
    avg_test_acc = test_correct / test_total

    macro_f1 = f1_score(all_labels, all_preds, average="macro")
    weighted_f1 = f1_score(all_labels, all_preds, average="weighted")
    macro_prec = precision_score(all_labels, all_preds, average="macro", zero_division=0)
    macro_rec  = recall_score(all_labels, all_preds, average="macro", zero_division=0)

    metrics = {
        "run_name": run_name,
        "best_val_acc": float(best_val_acc),
        "test_loss": float(avg_test_loss),
        "test_acc": float(avg_test_acc),
        "macro_f1": float(macro_f1),
        "weighted_f1": float(weighted_f1),
        "macro_precision": float(macro_prec),
        "macro_recall": float(macro_rec),
        "training_time_sec": float(train_time_sec),
        "best_checkpoint": best_ckpt_path,
        "config": ckpt["config"]
    }

    print(f"\nTEST RESULTS | Loss: {avg_test_loss:.4f} | Accuracy: {avg_test_acc*100:.2f}%\n")
    print("Classification Report:")
    print(classification_report(all_labels, all_preds, target_names=le.classes_))
    print("Confusion Matrix (numbers):")
    print(confusion_matrix(all_labels, all_preds))

    # Colored confusion matrix diagram (required)
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(7,6))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
    disp.plot(values_format="d")  # colored by default
    plt.title(f"Confusion Matrix (Counts) — {run_name}")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    # Save outputs (6)
    pred_df = pd.DataFrame({
        "text": X_test,
        "true_label": [le.classes_[i] for i in y_test],
        "pred_label": [le.classes_[i] for i in all_preds]
    })
    pred_path = os.path.join(save_dir, f"{run_name}_test_predictions.csv")
    pred_df.to_csv(pred_path, index=False)

    metrics_path = os.path.join(save_dir, f"{run_name}_metrics.json")
    with open(metrics_path, "w") as f:
        json.dump(metrics, f, indent=2)

    history_path = os.path.join(save_dir, f"{run_name}_history.csv")
    pd.DataFrame(history).to_csv(history_path, index=False)

    print("✅ Saved predictions:", pred_path)
    print("✅ Saved metrics:", metrics_path)
    print("✅ Saved history:", history_path)

    return metrics, history

set_seed(42)

baseline_metrics, baseline_history = run_bert_experiment(
    run_name="bert_baseline",
    max_len=100,
    batch_size=32,
    epochs=3,
    lr=2e-5,
    dropout=0.30,
    unfreeze_last_n=2,
    save_dir="results_bert/baseline"
)

manual_space = [
    {"lr": 2e-5, "max_len": 100, "dropout": 0.30},
    {"lr": 3e-5, "max_len": 100, "dropout": 0.30},
    {"lr": 2e-5, "max_len": 128, "dropout": 0.30},
    {"lr": 2e-5, "max_len": 100, "dropout": 0.40},
]

manual_results = []

for i, cfg in enumerate(manual_space, start=1):
    set_seed(42)
    run_name = f"bert_manual_{i}_lr{cfg['lr']}_len{cfg['max_len']}_do{cfg['dropout']}"
    metrics, _ = run_bert_experiment(
        run_name=run_name,
        max_len=cfg["max_len"],
        batch_size=32,
        epochs=2,              # short for tuning
        lr=cfg["lr"],
        dropout=cfg["dropout"],
        unfreeze_last_n=2,
        save_dir="results_bert/manual_search"
    )
    manual_results.append(metrics)

manual_df = pd.DataFrame(manual_results).sort_values("best_val_acc", ascending=False)
manual_df[["run_name","best_val_acc","test_acc","macro_f1","training_time_sec"]]

import optuna

def objective(trial):
    set_seed(42)

    lr = trial.suggest_float("lr", 1e-5, 5e-5, log=True)
    max_len = trial.suggest_categorical("max_len", [100, 128])
    dropout = trial.suggest_float("dropout", 0.20, 0.50)

    run_name = f"bert_optuna_lr{lr:.2e}_len{max_len}_do{dropout:.2f}"
    metrics, _ = run_bert_experiment(
        run_name=run_name,
        max_len=max_len,
        batch_size=32,
        epochs=2,           # short for tuning
        lr=lr,
        dropout=dropout,
        unfreeze_last_n=2,
        save_dir="results_bert/optuna_search"
    )
    return metrics["best_val_acc"]

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=3)   # increase to 8–12 if you have time

print("Best params:", study.best_params)
print("Best best_val_acc:", study.best_value)

best = {
    "lr": 3.436955010271621e-05,
    "max_len": 128,
    "dropout": 0.29545703432359016
}

set_seed(42)

final_metrics, final_history = run_bert_experiment(
    run_name="bert_final_best",
    max_len=best["max_len"],
    batch_size=32,
    epochs=5,          # or 8–10 if you want
    lr=best["lr"],
    dropout=best["dropout"],
    unfreeze_last_n=2,
    save_dir="results_bert/final"
)

import json
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModel
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

# ---- 1) Path to your best checkpoint
CKPT_PATH = "results_bert/final/bert_final_best_best.pt"

# ---- 2) Load checkpoint
ckpt = torch.load(CKPT_PATH, map_location=device, weights_only=False)
config = ckpt["config"]
label_classes = ckpt["label_classes"]

print("✅ Loaded checkpoint:", CKPT_PATH)
print("Config:", config)

# ---- 3) Restore label encoder
le = LabelEncoder()
le.classes_ = label_classes

# ---- 4) Rebuild tokenizer + base model
tokenizer = AutoTokenizer.from_pretrained(config["model_name"])
base_model = AutoModel.from_pretrained(config["model_name"])

# ---- 5) Rebuild your classifier model (MUST match your training architecture)
class MyModel(nn.Module):
    def __init__(self, base_model, num_classes, dropout=0.30):
        super().__init__()
        self.base = base_model
        self.dropout1 = nn.Dropout(dropout)
        self.fc1 = nn.Linear(768, 256)
        self.act = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)
        self.fc2 = nn.Linear(256, num_classes)

    def forward(self, input_ids, attention_mask):
        out = self.base(input_ids=input_ids, attention_mask=attention_mask)
        cls = out.last_hidden_state[:, 0]
        x = self.dropout1(cls)
        x = self.fc1(x)
        x = self.act(x)
        x = self.dropout2(x)
        return self.fc2(x)

model = MyModel(base_model, num_classes=len(le.classes_), dropout=config["dropout"]).to(device)
model.load_state_dict(ckpt["model_state"])
model.eval()

# ---- 6) Build test_loader again (same MAX_LEN used in training)
class MyDataset(torch.utils.data.Dataset):
    def __init__(self, X, Y, tokenizer, max_len):
        self.X = X
        self.Y = Y
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        text = str(self.X[idx])
        label = int(self.Y[idx])

        enc = self.tokenizer(
            text,
            max_length=self.max_len,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        return {
            "input_ids": enc["input_ids"].squeeze(0),
            "attention_mask": enc["attention_mask"].squeeze(0),
            "labels": torch.tensor(label, dtype=torch.long),
            "text": text
        }

# IMPORTANT: assumes X_test and y_test already exist from your 50/25/25 split cell
test_ds = MyDataset(X_test, y_test, tokenizer, config["max_len"])
test_loader = DataLoader(test_ds, batch_size=config["batch_size"], shuffle=False)

# ---- 7) Loss (unweighted for reporting; you can also use weighted if you want)
criterion = nn.CrossEntropyLoss()

# ---- 8) Run testing
test_loss_sum, test_correct, test_total = 0.0, 0, 0
all_preds, all_labels, all_texts = [], [], []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        logits = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = criterion(logits, labels)

        test_loss_sum += loss.item() * labels.size(0)
        preds = torch.argmax(logits, dim=1)

        test_correct += (preds == labels).sum().item()
        test_total += labels.size(0)

        all_preds.extend(preds.cpu().numpy().tolist())
        all_labels.extend(labels.cpu().numpy().tolist())
        all_texts.extend(batch["text"])

avg_test_loss = test_loss_sum / test_total
avg_test_acc = test_correct / test_total

print(f"\n✅ TEST RESULTS | Loss: {avg_test_loss:.4f} | Accuracy: {avg_test_acc*100:.2f}%\n")

print("Classification Report:")
print(classification_report(all_labels, all_preds, target_names=le.classes_))

cm = confusion_matrix(all_labels, all_preds)
print("Confusion Matrix (numbers):")
print(cm)

# ---- 9) Colored confusion matrix diagram
plt.figure(figsize=(7, 6))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
disp.plot(values_format="d")
plt.title("Confusion Matrix (Counts) — bert_final_best (TEST)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# ---- 10) (Optional) Save predictions
pred_df = pd.DataFrame({
    "text": all_texts,
    "true_label": [le.classes_[i] for i in all_labels],
    "pred_label": [le.classes_[i] for i in all_preds],
})
pred_path = "results_bert/final/bert_final_best_TEST_predictions.csv"
pred_df.to_csv(pred_path, index=False)
print("✅ Saved test predictions to:", pred_path)